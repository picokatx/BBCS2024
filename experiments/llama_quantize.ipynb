{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061d446f2514cda8cd4b8303047fe0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64839880b0d44cabde5f445b340771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c40ea32a644d6d84e4a3ba46492615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_causal = LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 211.53it/s]\n",
      "100%|██████████| 155/155 [00:17<00:00,  9.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): HQQLinear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): HQQLinear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): HQQLinear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): HQQLinear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): HQQLinear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): HQQLinear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): HQQLinear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64)\n",
    "AutoHQQHFModel.quantize_model(llama_causal, quant_config=quant_config, \n",
    "                                    compute_dtype=torch.float16, \n",
    "                                    device=\"cuda\")\n",
    "# 1.4gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.save_quantized(llama_causal, \"./tinyllama_hqq_4_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 2586.31it/s]\n",
      "100%|██████████| 155/155 [00:00<00:00, 11347.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "llama_causal = AutoHQQHFModel.from_quantized(\"./tinyllama_hqq_4_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f465ba5ff8fd4277a0d9db3f4a590262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1478b6ae6d4fa7a51bb39e7daceeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a735d49d97465aafd85dd3f13a14f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c8a9dd6ca746b0b7b7c7a0ad714e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  529, 29989,  1792, 29989, 29958,    13, 10994, 29892,   920,   526,\n",
       "           366, 29973,     2, 29871,    13, 29966, 29989,   465, 22137, 29989,\n",
       "         29958,    13, 29902, 29915, 29885,  2599,  2107, 29889,  1128,   508,\n",
       "           306,  1371,   366,  9826, 29973,     2, 29871,    13, 29966, 29989,\n",
       "          1792, 29989, 29958,    13, 29902, 29915, 29881,   763,   304,  1510,\n",
       "          1283,   920, 13563,  1350,   572,  1218,  1736, 29991,     2, 29871,\n",
       "            13]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = llama_tokenizer.apply_chat_template([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are now an expert on recognizing emotions in conversations. Respond to any input messages with a classification of the message from the following labels: [angry, happy, other].\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"waaaaaa I dropped my ice cream :(((( this is the WORST DAY EVER!\",\n",
    "    },\n",
    "], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "gen_cfg.max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nYou are now an expert on recognizing emotions in conversations. Respond to any input messages with a classification of the message from the following labels: [angry, happy, other].</s> \\n<|user|>\\nwaaaaaa I dropped my ice cream :(((( this is the WORST DAY EVER!</s> \\n<|assistant|>\\nI am not able to experience emotions, but I can provide you with a response to your question.\\n\\nin this case, you dropped your ice cream, which is a common occurrence. The label \"angry\" would be appropriate for a message that describes the emotions you experienced when you dropped the ice cream. The label \"happy\" would be appropriate for a message that describes the emotions you experienced when you saw the ice cream on the ground.\\n\\nin this case, you could also use the label \"other\" to describe the emotions you experienced when you realized that you dropped your ice cream. This label would be appropriate for a message that describes the emotions you experienced when you realized that you dropped the ice cream.\\n\\nremember, emotions can be complex and subjective'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(llama_causal.generate(input_ids.cuda().long(), generation_config=gen_cfg)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
