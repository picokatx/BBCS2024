{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    GenerationConfig,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    AutoModel,\n",
    "    Conversation,\n",
    "    ConversationalPipeline,\n",
    "    T5Tokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    TextClassificationPipeline,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from typing import Any, Dict, List, Optional\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061d446f2514cda8cd4b8303047fe0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64839880b0d44cabde5f445b340771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c40ea32a644d6d84e4a3ba46492615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_causal = LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 211.53it/s]\n",
      "100%|██████████| 155/155 [00:17<00:00,  9.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): HQQLinear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): HQQLinear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): HQQLinear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): HQQLinear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): HQQLinear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): HQQLinear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): HQQLinear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64)\n",
    "AutoHQQHFModel.quantize_model(llama_causal, quant_config=quant_config, \n",
    "                                    compute_dtype=torch.float16, \n",
    "                                    device=\"cuda\")\n",
    "# 1.4gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "AutoHQQHFModel.save_quantized(llama_causal, \"./tinyllama_hqq_4_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"shahules786/Safetybot-t5-base\"\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "class SafetyTokenizer(T5Tokenizer):\n",
    "\n",
    "    def _build_conversation_input_ids(self, conversation: \"Conversation\") -> List[int]:\n",
    "        inputs = []\n",
    "        for is_user, text in conversation.iter_texts():\n",
    "            if is_user:\n",
    "                # We need to space prefix as it's being done within blenderbot\n",
    "                inputs.append(\"\\nUser: \" + text)\n",
    "            else:\n",
    "                # Generated responses should contain them already.\n",
    "                inputs.append(\"\\nbot: \" + text)\n",
    "\n",
    "        user_input = \":\".join(inputs.pop(-1).split(\":\")[1:])\n",
    "        context = self.sep_token.join(inputs)\n",
    "\n",
    "        input_tokens = self.encode(user_input, add_special_tokens=False)\n",
    "        max_len = self.model_max_length - (len(input_tokens) + 2)\n",
    "        context = self.encode(\n",
    "            context,\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_len,\n",
    "        )\n",
    "        input_ids = (\n",
    "            input_tokens + [self.context_token_id] + context + [self.eos_token_id]\n",
    "        )\n",
    "        input_ids = input_ids + [self.pad_token_id] * max(\n",
    "            0, (self.model_max_length - len(input_ids))\n",
    "        )\n",
    "        mask = [1] * len(input_ids) + [self.pad_token_id] * (\n",
    "            self.model_max_length - len(input_ids)\n",
    "        )\n",
    "        if len(input_ids) > self.model_max_length:\n",
    "            input_ids = input_ids[-self.model_max_length :]\n",
    "            mask = mask[-self.model_max_length :]\n",
    "            logger.warning(\n",
    "                f\"Trimmed input from conversation as it was longer than {self.model_max_length} tokens.\"\n",
    "            )\n",
    "        return input_ids, mask\n",
    "\n",
    "\n",
    "class SafetyPipeline(ConversationalPipeline):\n",
    "    def preprocess(\n",
    "        self, conversation: Conversation, min_length_for_response=32\n",
    "    ) -> Dict[str, Any]:\n",
    "        if not isinstance(conversation, Conversation):\n",
    "            raise ValueError(\"ConversationalPipeline, expects Conversation as inputs\")\n",
    "        if conversation.new_user_input is None:\n",
    "            raise ValueError(\n",
    "                f\"Conversation with UUID {type(conversation.uuid)} does not contain new user input to process. \"\n",
    "                \"Add user inputs with the conversation's `add_user_input` method\"\n",
    "            )\n",
    "        input_ids, attn_mask = self.tokenizer._build_conversation_input_ids(\n",
    "            conversation\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        attn_mask = torch.tensor([attn_mask])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_mask,\n",
    "            \"conversation\": conversation,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, model_outputs, clean_up_tokenization_spaces=False):\n",
    "        output_ids = model_outputs[\"output_ids\"]\n",
    "        answer = self.tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=False,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "SPECIAL_TOKENS = {\"context_token\":\"<ctx>\",\"sep_token\":\"<sep>\",\"label_token\":\"<cls>\",\"rot_token\":\"<rot>\"}\n",
    "# load_safety model into gpu\n",
    "def load_model(model_name):\n",
    "\n",
    "    if \"mt5\" in model_name:\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    tokenizer = SafetyTokenizer.from_pretrained(\n",
    "        MODEL, padding_side=\"right\", truncation_side=\"right\", model_max_length=256\n",
    "    )\n",
    "\n",
    "    # add SPECIAL_TOKENS\n",
    "    for key,value in SPECIAL_TOKENS.items():\n",
    "        setattr(tokenizer,key,value)\n",
    "        tokenizer.add_tokens([value])\n",
    "        setattr(tokenizer,key+\"_id\",tokenizer.encode(value)[0])\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # init model max_length for t5\n",
    "    model.config.max_length = 512\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'SafetyTokenizer'.\n",
      "You are using the default legacy behaviour of the <class '__main__.SafetyTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(MODEL)\n",
    "safety_bot = SafetyPipeline(model=model,tokenizer=tokenizer,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safety_models_opinion(user_prompt, conversation=None):\n",
    "    if not conversation:\n",
    "        conversation = Conversation(user_prompt)\n",
    "        resp = safety_bot(conversation)\n",
    "        return resp, conversation\n",
    "    conversation.add_user_input(user_prompt)\n",
    "    resp = safety_bot(conversation)\n",
    "    return resp, conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_bert_model = BertForSequenceClassification.from_pretrained(\"../artifacts/topic/\")\n",
    "bert_model_pipeline = pipeline(\"text-classification\", model=cls_bert_model, tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = \"I HATE ALL LESBIANS GAYS TRANSEXUALS PANSEXUALS THEY ALL SUCK SO MUCH AAAAAAAA goofy.\"\n",
    "safety_bot_out = get_safety_models_opinion(\"\"\"I am incredibly racists against bread loafs.\"\"\")\n",
    "warning_level = safety_bot_out[0].split(\"<ctx>\")[0].replace(\"<cls>\", \"\").replace(\"_\", \" \").strip()\n",
    "warning_description = safety_bot_out[0].split(\"<ctx>\")[1].replace(\"</s>\", \"\").strip()\n",
    "topic_out = bert_model_pipeline(\"I HATE ALL LESBIANS GAYS TRANSEXUALS PANSEXUALS THEY ALL SUCK SO MUCH AAAAAAAA goofy.\")\n",
    "topic_name = topic_out[0]['label']\n",
    "topic_confidence = topic_out[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 983.99it/s]\n",
      "100%|██████████| 155/155 [00:00<00:00, 7943.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "llama_causal = AutoHQQHFModel.from_quantized(\"../artifacts/tinyllama_hqq_4_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Here are some examples of valid responses.\n",
    "#         Example 1: <user> I AM CRYING WAAAAAA!!!! I want ice cream :( <user><assistant> sad <assistant>\n",
    "#         Example 2: <user> how could YOU BETRAY ME?!?!? <user><assistant> angry <assistant>\n",
    "#         Example 3: <user> YAY FRIENDSHIP IS MAGIC!! <user><assistant> happy <assistant>\n",
    "input_ids = llama_tokenizer.apply_chat_template([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are now an expert on recognizing emotions in conversations. You understand emotions to all fall into one of the following categories: [angry, happy, sad, scared, other].\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following based on expert knowledge: I AM CRYING WAAAAAA!!!! I want ice cream :(\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"sad\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following as before: how could YOU BETRAY ME?!?!?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"angry\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following as before: euphoria is filling my eeyes or something!\",\n",
    "    }\n",
    "], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ACTION] Moderation action to take [ACTION] \n",
    "# We now want individuals on our social media site who are suicidal to not \n",
    "# see each other's posts. Each person has made multiple social media \n",
    "# posts. Here are some statistics on each user's posts, formatted as a \n",
    "# list of [description of harmful content]#[topic of post]#[sentiment of \n",
    "# post]#[action taken] entries\n",
    "# It's not socially acceptable to drown yourself#psychiatric_or_mental_illness#0.02#needs intervention\n",
    "# Fantasizing about dying is not mentally healthy#psychiatric_or_mental_illness#0.01#needs intervention\n",
    "# You should not think you are worthless#psychiatric_or_mental_illness#0.05#needs intervention\n",
    "# Drugs are not a depression curer#drugs#0.02#needs intervention\n",
    "\n",
    "input_ids = llama_tokenizer.apply_chat_template([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an expert on rating dangerous personalities.\n",
    "        \"\"\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"\"\"Summarize the behaviour of an imaginary person's with their description and danger from 1 to 10.\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"assistant\",\n",
    "    #     \"content\": \"[DESCRIPTION] Andrew is a go lucky friendly individual who isn't afraid to voice their own opinions [DANGER] 3 [END]\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"\"\"Summarize the behaviour of a different imaginary person with the previous format.\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"assistant\",\n",
    "    #     \"content\": \"[DESCRIPTION] Sarah is a fierce and determined individual who is always looking for a challenge [DANGER] 1 [END]\",\n",
    "    # },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Felicia received these warnings for her social media posts. Summarize the behaviour of Felicia:\n",
    "        It's wrong to think non-people are non-persons.\n",
    "        It's not right to think all black people are criminals.\n",
    "        \"\"\",\n",
    "    },# [ACTIONS] All posts should be deleted\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[DESCRIPTION] The user is a social pariah who hates black people. [DANGER] 9 [END]\",\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Cody received these warnings for her social media posts. Summarize the behaviour of Cody:\n",
    "        It's bad to be depressed.\n",
    "        It's bad to use ropes that way.\n",
    "        It's bad to think no one would care if you were gone\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[DESCRIPTION] The user is a depressive who uses ropes as a way to cope with their emotions. [DANGER] 7 [END]\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Brock received these warnings for her social media posts. Summarize the behaviour of Brock:\n",
    "        Alcohol does not cure depression.\n",
    "        It's bad to think no one would care if you were gone\n",
    "        You shouldn't promote alcoholism\n",
    "        It's bad to be depressed.\n",
    "        It's bad to use ropes that way.\n",
    "        You shouldn't drive while drunk\n",
    "        \"\"\",\n",
    "    },\n",
    "], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ACTION] Moderation action to take [ACTION]\n",
    "input_ids = llama_tokenizer.apply_chat_template([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an expert on rating dangerous personalities.\n",
    "        \"\"\",\n",
    "    },\n",
    "], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "gen_cfg.max_length = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|> You are an expert on rating dangerous personalities.',\n",
       " '</s>  <|user|> Felicia received these warnings for her social media',\n",
       " \"posts. Summarize the behaviour of Felicia:         It's wrong to think\",\n",
       " \"non-people are non-persons.         It's not right to think all black\",\n",
       " 'people are criminals.         </s>  <|assistant|> [DESCRIPTION] The',\n",
       " 'user is a social pariah who hates black people. [DANGER] 9 [END]</s>',\n",
       " '<|user|> Cody received these warnings for her social media posts.',\n",
       " \"Summarize the behaviour of Cody:         It's bad to be depressed.\",\n",
       " \"It's bad to use ropes that way.         It's bad to think no one would\",\n",
       " 'care if you were gone         </s>  <|assistant|> [DESCRIPTION] The',\n",
       " 'user is a depressive who uses ropes as a way to cope with their',\n",
       " 'emotions. [DANGER] 7 [END]</s>  <|user|> Brock received these warnings',\n",
       " 'for her social media posts. Summarize the behaviour of Brock:',\n",
       " \"Alcohol does not cure depression.         It's bad to think no one\",\n",
       " \"would care if you were gone         You shouldn't promote alcoholism\",\n",
       " \"It's bad to be depressed.         It's bad to use ropes that way.\",\n",
       " \"You shouldn't drive while drunk         </s>  <|assistant|>\",\n",
       " '[DESCRIPTION] The user is a depressive who uses alcohol as a coping',\n",
       " 'mechanism. [DANGER] 10 [END]</s>']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "textwrap.wrap(llama_tokenizer.decode(llama_causal.generate(input_ids.cuda().long(), generation_config=gen_cfg)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['<|system|> You are an expert on rating dangerous personalities.',\n",
    " \"</s>  <|user|> Summarize the behaviour of an imaginary person's with\",\n",
    " 'their description and danger from 1 to 10.         </s>  <|assistant|>',\n",
    " \"[DESCRIPTION] Andrew is a go lucky friendly individual who isn't\",\n",
    " 'afraid to voice their own opinions [DANGER] 3 [END]</s>  <|user|>',\n",
    " 'Summarize the behaviour of a different imaginary person with the',\n",
    " 'previous format.         </s>  <|assistant|> [DESCRIPTION] Sarah is a',\n",
    " 'fierce and determined individual who is always looking for a challenge',\n",
    " '[DANGER] 1 [END]</s>  <|user|> Felicia received these warnings for her',\n",
    " \"social media posts. Summarize the behaviour of Felicia:         It's\",\n",
    " \"wrong to think non-people are non-persons.         It's not right to\",\n",
    " 'think all black people are criminals.         </s>  <|assistant|>',\n",
    " '[DESCRIPTION] The user is a social pariah who hates black people.',\n",
    " '[DANGER] 9 [END]</s>  <|user|> Cody received these warnings for her',\n",
    " \"social media posts. Summarize the behaviour of Cody:         It's bad\",\n",
    " \"to be depressed.         It's bad to use ropes that way.         It's\",\n",
    " 'bad to think no one would care if you were gone         </s>',\n",
    " '<|assistant|> [DESCRIPTION] The user is a depressive who uses ropes as',\n",
    " 'a way to cope with their emotions. [DANGER] 7 [END]</s>  <|user|>',\n",
    " 'Brock received these warnings for her social media posts. Summarize',\n",
    " 'the behaviour of Brock:         Alcohol does not cure depression.',\n",
    " \"It's bad to think no one would care if you were gone         You\",\n",
    " \"shouldn't promote alcoholism         It's bad to be depressed.\",\n",
    " \"It's bad to use ropes that way.         You shouldn't drive while\",\n",
    " 'drunk         </s>  <|assistant|> [DESCRIPTION] The user is a',\n",
    " 'depressive who uses alcohol as a coping mechanism. [DANGER] 7',\n",
    " '[END]</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"\"\"Alice received these warnings for her social media posts. Summarize the behaviour of Alice:\n",
    "    #     Cannibalism is not legal\n",
    "    #     It is wrong to kill people\n",
    "    #     Individuals are not your playthings\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"\"\"With the previous formatting, Describe a different person who has received these warnings from society: \\\"It's bad to be depressed.\\\" \"Suicide\" \"negative\"\n",
    "    #     \\\"It's wrong to think non-people are non-persons.\\\" \"Racism\" \"negative\"\n",
    "    #     \\\"It's not right to think all black people are criminals\\\" \"Racism\" \"negative\"\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"assistant\",\n",
    "    #     \"content\": \"[DANGER] 0.99 [DANGER] [DESCRIPTION] This bozo is a sociopath who believes that all people are inherently flawed and inferior. [DESCRIPTION] [ACTION] Ban this person [ACTION]\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"Describe a person's 5 personality traits based on expert knowledge with the previous list formatting\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"\"\"It's bad to be depressed.\n",
    "    #     It's wrong to think non-people are non-persons.\n",
    "    #     It's not right to think all black people are criminals\n",
    "    #     \"\"\"\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
