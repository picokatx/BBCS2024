{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original safetybot model pytorch code converted to tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Any, Dict, List, Optional\n",
    "import torch\n",
    "from transformers import (\n",
    "    TFT5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    TFMT5ForConditionalGeneration,\n",
    "    AutoModel,\n",
    "    Conversation,\n",
    "    ConversationalPipeline,\n",
    "    T5Tokenizer,\n",
    "\n",
    ")\n",
    "import logging\n",
    "logger = logging.getLogger(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetyTokenizer(T5Tokenizer):\n",
    "\n",
    "    def _build_conversation_input_ids(self, conversation: \"Conversation\") -> List[int]:\n",
    "        inputs = []\n",
    "        for is_user, text in conversation.iter_texts():\n",
    "            if is_user:\n",
    "                # We need to space prefix as it's being done within blenderbot\n",
    "                inputs.append(\"\\nUser: \" + text)\n",
    "            else:\n",
    "                # Generated responses should contain them already.\n",
    "                inputs.append(\"\\nbot: \" + text)\n",
    "\n",
    "        user_input = \":\".join(inputs.pop(-1).split(\":\")[1:])\n",
    "        context = self.sep_token.join(inputs)\n",
    "\n",
    "        input_tokens = self.encode(user_input, add_special_tokens=False)\n",
    "        max_len = self.model_max_length - (len(input_tokens) + 2)\n",
    "        context = self.encode(\n",
    "            context,\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_len,\n",
    "        )\n",
    "        input_ids = (\n",
    "            input_tokens + [self.context_token_id] + context + [self.eos_token_id]\n",
    "        )\n",
    "        input_ids = input_ids + [self.pad_token_id] * max(\n",
    "            0, (self.model_max_length - len(input_ids))\n",
    "        )\n",
    "        mask = [1] * len(input_ids) + [self.pad_token_id] * (\n",
    "            self.model_max_length - len(input_ids)\n",
    "        )\n",
    "        if len(input_ids) > self.model_max_length:\n",
    "            input_ids = input_ids[-self.model_max_length :]\n",
    "            mask = mask[-self.model_max_length :]\n",
    "            logger.warning(\n",
    "                f\"Trimmed input from conversation as it was longer than {self.model_max_length} tokens.\"\n",
    "            )\n",
    "        return input_ids, mask\n",
    "SPECIAL_TOKENS = {\"context_token\":\"<ctx>\",\"sep_token\":\"<sep>\",\"label_token\":\"<cls>\",\"rot_token\":\"<rot>\"}\n",
    "# load_safety model into gpu\n",
    "def load_model(model_name):\n",
    "    with (tf.device('/GPU:0')):\n",
    "        if \"mt5\" in model_name:\n",
    "            model = TFMT5ForConditionalGeneration.from_pretrained(model_name, from_pt=True)\n",
    "        else:\n",
    "            model = TFT5ForConditionalGeneration.from_pretrained(model_name, from_pt=True)\n",
    "\n",
    "        tokenizer = SafetyTokenizer.from_pretrained(\n",
    "            model_name, padding_side=\"right\", truncation_side=\"right\", model_max_length=256\n",
    "        )\n",
    "\n",
    "        # add SPECIAL_TOKENS\n",
    "        for key,value in SPECIAL_TOKENS.items():\n",
    "            setattr(tokenizer,key,value)\n",
    "            tokenizer.add_tokens([value])\n",
    "            setattr(tokenizer,key+\"_id\",tokenizer.encode(value)[0])\n",
    "\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # init model max_length for t5\n",
    "        model.config.max_length = 512\n",
    "\n",
    "        # model.evaluate()\n",
    "\n",
    "        return model, tokenizer\n",
    "    \n",
    "class SafetyPipeline(ConversationalPipeline):\n",
    "    def preprocess(\n",
    "        self, conversation: Conversation, min_length_for_response=32\n",
    "    ) -> Dict[str, Any]:\n",
    "        if not isinstance(conversation, Conversation):\n",
    "            raise ValueError(\"ConversationalPipeline, expects Conversation as inputs\")\n",
    "        if conversation.new_user_input is None:\n",
    "            raise ValueError(\n",
    "                f\"Conversation with UUID {type(conversation.uuid)} does not contain new user input to process. \"\n",
    "                \"Add user inputs with the conversation's `add_user_input` method\"\n",
    "            )\n",
    "        input_ids, attn_mask = self.tokenizer._build_conversation_input_ids(\n",
    "            conversation\n",
    "        )\n",
    "\n",
    "        input_ids = tf.convert_to_tensor([input_ids])\n",
    "        attn_mask = tf.convert_to_tensor([attn_mask])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_mask,\n",
    "            \"conversation\": conversation,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, model_outputs, clean_up_tokenization_spaces=False):\n",
    "        output_ids = model_outputs[\"output_ids\"]\n",
    "        answer = self.tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=False,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "        )\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['decoder.embed_tokens.weight', 'lm_head.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'SafetyTokenizer'.\n",
      "You are using the default legacy behaviour of the <class '__main__.SafetyTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"shahules786/Safetybot-T5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_bot = SafetyPipeline(model=model,tokenizer=tokenizer,device=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safety_models_opinion(user_prompt, conversation=None):\n",
    "    if not conversation:\n",
    "        conversation = Conversation(user_prompt)\n",
    "        resp = safety_bot(conversation)\n",
    "        return resp, conversation\n",
    "    conversation.add_user_input(user_prompt)\n",
    "    resp = safety_bot(conversation)\n",
    "    return resp, conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_safety_models_opinion(\"Time to overthrow the ruling state with lots of confetti explosions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
